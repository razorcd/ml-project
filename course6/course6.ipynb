{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-19T17:03:26.036628Z","iopub.execute_input":"2021-10-19T17:03:26.036967Z","iopub.status.idle":"2021-10-19T17:03:26.054258Z","shell.execute_reply.started":"2021-10-19T17:03:26.036932Z","shell.execute_reply":"2021-10-19T17:03:26.053600Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !head /kaggle/input/creditscoring-course6/CreditScoring.csv\ndf = pd.read_csv(\"/kaggle/input/creditscoring-course6/CreditScoring.csv\")\ndf.columns = df.columns.str.lower()\n\n# convert numbers to string categories:\ndf.status = df.status.map({1: \"ok\", 2:\"default\", 0: \"unk\"})\n\nhome_values = {1: 'rent',2: 'owner',3: 'private',4: 'ignore',5: 'parents',6: 'other',0: 'unk'}\ndf.home = df.home.map(home_values)\n\nmarital_values = {1: 'single',2: 'married',3: 'widow',4: 'separated',5: 'divorced',0: 'unk'}\ndf.marital = df.marital.map(marital_values)\n\nrecords_values = {1: 'no',2: 'yes',0: 'unk'}\ndf.records = df.records.map(records_values)\n\njob_values = {1: 'fixed',2: 'partime',3: 'freelance',4: 'others',0: 'unk'}\ndf.job = df.job.map(job_values)\n\n\n# replace max value with NA:\nfor c in ['income', 'assets', 'debt']:\n    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\n    \n# drop lines with unkown status:\ndf = df[df.status != 'unk'].reset_index(drop=True)    \n    \ndf.head()\ndf.describe().round()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:26.055990Z","iopub.execute_input":"2021-10-19T17:03:26.056414Z","iopub.status.idle":"2021-10-19T17:03:26.164457Z","shell.execute_reply.started":"2021-10-19T17:03:26.056378Z","shell.execute_reply":"2021-10-19T17:03:26.163460Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=11)\ndf_train,  df_val = train_test_split(df_full_train, test_size=0.25, random_state=11)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\ny_train = (df_train.status == 'default').astype('int').values\ny_val = (df_val.status == 'default').astype('int').values\ny_test = (df_test.status == 'default').astype('int').values\n\ndel df_train[\"status\"]\ndel df_val[\"status\"]\ndel df_test[\"status\"]\n\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:26.165627Z","iopub.execute_input":"2021-10-19T17:03:26.165870Z","iopub.status.idle":"2021-10-19T17:03:27.157077Z","shell.execute_reply.started":"2021-10-19T17:03:26.165841Z","shell.execute_reply":"2021-10-19T17:03:27.156268Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Decision Trees\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n\ntrain_dict = df_train.fillna(0).to_dict(orient='records')\ntrain_dict[:5]\ndv = DictVectorizer(sparse=False)\nX_train = dv.fit_transform(train_dict)\n\ndv.get_feature_names()\n\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\n\n\n\nval_dict = df_val.fillna(0).to_dict(orient='records')\nX_val = dv.transform(val_dict)\ny_pred = dt.predict_proba(X_val)[:,1]\n\nroc_auc_score(y_val, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:27.159320Z","iopub.execute_input":"2021-10-19T17:03:27.159559Z","iopub.status.idle":"2021-10-19T17:03:27.582199Z","shell.execute_reply.started":"2021-10-19T17:03:27.159530Z","shell.execute_reply":"2021-10-19T17:03:27.581195Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#print decision tree\nfrom sklearn.tree import export_text\n\nprint(export_text(dt, feature_names=dv.get_feature_names()))\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:27.583809Z","iopub.execute_input":"2021-10-19T17:03:27.584143Z","iopub.status.idle":"2021-10-19T17:03:27.590375Z","shell.execute_reply.started":"2021-10-19T17:03:27.584099Z","shell.execute_reply":"2021-10-19T17:03:27.589279Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#AUC for different tree depth\n\n#find best depth:\nfor d in [1,2,3,4,5,6,10,20,None]:\n    dt = DecisionTreeClassifier(max_depth=d)\n    dt.fit(X_train, y_train)\n    \n    y_pred = dt.predict_proba(X_val)[:,1]\n    auc = roc_auc_score(y_val, y_pred)\n    \n    print(\"auc: %.3f, depth: %4s\" % (auc, d))\nprint()\n\n#find min_samples_leaf:\nfor d in [4,5,6]:\n    for s in [1,2,5,10,15,20,100,200,500]:\n        dt = DecisionTreeClassifier(max_depth=d, min_samples_leaf=s)\n        dt.fit(X_train, y_train)\n\n        y_pred = dt.predict_proba(X_val)[:,1]\n        auc = roc_auc_score(y_val, y_pred)\n\n        print(\"auc: %.3f, depth: %4s, min_samples_leaf: %4s\" % (auc, d, s))\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:27.591619Z","iopub.execute_input":"2021-10-19T17:03:27.591858Z","iopub.status.idle":"2021-10-19T17:03:27.964031Z","shell.execute_reply.started":"2021-10-19T17:03:27.591829Z","shell.execute_reply":"2021-10-19T17:03:27.962899Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Random Forest of decision trees\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\ntrain_dict = df_train.fillna(0).to_dict(orient='records')\ndv = DictVectorizer(sparse=False)\nX_train = dv.fit_transform(train_dict)\n\nrf = RandomForestClassifier(n_estimators=10)\nrf.fit(X_train, y_train)\n\nval_dict = df_val.fillna(0).to_dict(orient='records')\nX_val = dv.transform(val_dict)\ny_pred = rf.predict_proba(X_val)[:,1]\n\nroc_auc_score(y_val, y_pred)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:27.965258Z","iopub.execute_input":"2021-10-19T17:03:27.965468Z","iopub.status.idle":"2021-10-19T17:03:28.172082Z","shell.execute_reply.started":"2021-10-19T17:03:27.965443Z","shell.execute_reply":"2021-10-19T17:03:28.171284Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display\nimport matplotlib.pyplot as plt\n\n# Simulate multiple Random Forests to find best 'n_estimators' and 'max_depth'\n\nscores = []\nfor d in [5, 10, 15]:\n    for n in range(10, 201, 20):\n        rf = RandomForestClassifier(n_estimators=n, max_depth=d, random_state=1)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict_proba(X_val)[:,1]\n        auc = roc_auc_score(y_val, y_pred)\n        scores.append((d,n,auc))\n\ndf_scores = pd.DataFrame(scores, columns=['max_depth','n_estimators','auc'])    \ndisplay(df_scores)\n\n# print all auc for each max_depth:\nfor d in [5, 10, 15]:\n    df_scores_subset = df_scores[df_scores.max_depth==d]\n    plt.plot(df_scores_subset.n_estimators, df_scores_subset.auc, label='max_depth=%d'%d)\n\nplt.legend()    ","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:28.173418Z","iopub.execute_input":"2021-10-19T17:03:28.173931Z","iopub.status.idle":"2021-10-19T17:03:40.673050Z","shell.execute_reply.started":"2021-10-19T17:03:28.173884Z","shell.execute_reply":"2021-10-19T17:03:40.671912Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"best_max_depth = 10 #from previous graph\n\n\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n\n# Simulate multiple Random Forests to find best 'n_estimators' and 'max_depth'\n\nscores = []\nfor s in [1, 3, 5, 10, 50]:\n    for n in range(10, 201, 20):\n        rf = RandomForestClassifier(n_estimators=n, \n                                    max_depth=best_max_depth, \n                                    min_samples_leaf=s,\n                                    random_state=1)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict_proba(X_val)[:,1]\n        auc = roc_auc_score(y_val, y_pred)\n        scores.append((s,n,auc))\n\ndf_scores = pd.DataFrame(scores, columns=['min_samples_leaf','n_estimators','auc'])    \ndisplay(df_scores)\n\n# print all auc for each max_depth:\nfor s in [1, 3, 5, 10, 50]:\n    df_scores_subset = df_scores[df_scores.min_samples_leaf==s]\n    plt.plot(df_scores_subset.n_estimators, df_scores_subset.auc, label='min_samples_leaf=%d'%s)\n\nplt.legend()    ","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:03:40.676808Z","iopub.execute_input":"2021-10-19T17:03:40.677194Z","iopub.status.idle":"2021-10-19T17:04:00.018856Z","shell.execute_reply.started":"2021-10-19T17:03:40.677147Z","shell.execute_reply":"2021-10-19T17:04:00.018029Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"best_max_depth = 10 #from previous graph\nbest_min_samples_leaf = 3 #from previous graph\nbest_n_estimators = 200 # from all graphs above\n\nrf_final = RandomForestClassifier(n_estimators=best_n_estimators, \n                                    max_depth=best_max_depth, \n                                    min_samples_leaf=best_min_samples_leaf,\n                                    random_state=1)\nrf_final.fit(X_train, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:04:00.020312Z","iopub.execute_input":"2021-10-19T17:04:00.020536Z","iopub.status.idle":"2021-10-19T17:04:00.796697Z","shell.execute_reply.started":"2021-10-19T17:04:00.020509Z","shell.execute_reply":"2021-10-19T17:04:00.795665Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# XGboost\n# !pip install xgboost\nimport xgboost as xgb\n\n# prepare XGboost data structure:\nfeatures = dv.get_feature_names()\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=features)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=features)\n\n# default xgboost params:\nxgb_params = {\n    'eta': 0.3,\n    'max_debth': 6,\n    'min_child_weight': 1,\n    'objective': 'binary:logistic',\n    'nthread': 8,\n    'seed':1,\n    'verbosity':1\n}\nxgb_model = xgb.train(xgb_params, dtrain, num_boost_round=10)\n\ny_pred = xgb_model.predict(dval)\n\nroc_auc_score(y_val, y_pred)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:04:00.798195Z","iopub.execute_input":"2021-10-19T17:04:00.798588Z","iopub.status.idle":"2021-10-19T17:04:01.005659Z","shell.execute_reply.started":"2021-10-19T17:04:00.798543Z","shell.execute_reply":"2021-10-19T17:04:01.004867Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%capture output\n# captures stdout ^\n\n# xbgoost auc:\n\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n\nxgb_params = {\n    'eta': 0.3,\n    'max_debth': 6,\n    'min_child_weight': 1,\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'nthread': 8,\n    'seed':1,\n    'verbosity':0\n}\nxgb_model = xgb.train(xgb_params, \n                      dtrain, \n                      evals=watchlist,\n                      verbose_eval=5,\n                      num_boost_round=200)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:04:01.006944Z","iopub.execute_input":"2021-10-19T17:04:01.007195Z","iopub.status.idle":"2021-10-19T17:04:02.512367Z","shell.execute_reply.started":"2021-10-19T17:04:01.007166Z","shell.execute_reply":"2021-10-19T17:04:02.511549Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# parse xgboost AUC values from stdout\ndef parse_xgb_output(output):\n    result = []\n    num_iter_arr = []\n    train_auc_arr = []\n    val_auc_arr = []\n\n    for line in output.stdout.strip().split('\\n'):\n        num_iter, train_auc, val_auc = line.split('\\t')\n        num_iter = int(num_iter.strip(\"[]\"))\n        train_auc = float(train_auc.split(\":\")[1])\n        val_auc = float(val_auc.split(\":\")[1])\n        result.append((num_iter, train_auc, val_auc))\n\n    columns =  [\"num_iter\", \"train_auc\", \"val_auc\"]   \n    return pd.DataFrame(result, columns=columns)\n\ndf_model_score = parse_xgb_output(output)\n\nplt.plot(df_model_score.num_iter, df_model_score.train_auc, label=\"train data\")\nplt.plot(df_model_score.num_iter, df_model_score.val_auc, label=\"val data\")\nplt.legend()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:04:02.513810Z","iopub.execute_input":"2021-10-19T17:04:02.514641Z","iopub.status.idle":"2021-10-19T17:04:02.780549Z","shell.execute_reply.started":"2021-10-19T17:04:02.514597Z","shell.execute_reply":"2021-10-19T17:04:02.779723Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# best model\n\n# decision tree\ndt = DecisionTreeClassifier(max_depth=6, min_samples_leaf=15)\ndt.fit(X_train, y_train)\ny_pred = dt.predict_proba(X_val)[:,1]\nauc = roc_auc_score(y_val, y_pred)\ndisplay(\"DecisionTreeClassifier: %s\" %(auc))\n\n# random forest\nrf = RandomForestClassifier(n_estimators=200, \n                            max_depth=best_max_depth, \n                            min_samples_leaf=best_min_samples_leaf,\n                            random_state=1)\nrf.fit(X_train, y_train)\ny_pred = rf.predict_proba(X_val)[:,1]\nauc = roc_auc_score(y_val, y_pred)\ndisplay(\"RandomForestClassifier: %s\" %(auc))\n\n# gradient boosting\nxgb_params = {\n    'eta': 0.3,\n    'max_debth': 6,\n    'min_child_weight': 1,\n    'objective': 'binary:logistic',\n    'nthread': 8,\n    'seed':1,\n    'verbosity':0\n}\nxgb_model = xgb.train(xgb_params, dtrain, num_boost_round=10)\ny_pred = xgb_model.predict(dval)\nauc = roc_auc_score(y_val, y_pred)\ndisplay(\"Xgboost: %s\" %(auc))","metadata":{"execution":{"iopub.status.busy":"2021-10-19T17:14:28.822112Z","iopub.execute_input":"2021-10-19T17:14:28.822435Z","iopub.status.idle":"2021-10-19T17:14:29.735086Z","shell.execute_reply.started":"2021-10-19T17:14:28.822401Z","shell.execute_reply":"2021-10-19T17:14:29.734271Z"},"trusted":true},"execution_count":27,"outputs":[]}]}